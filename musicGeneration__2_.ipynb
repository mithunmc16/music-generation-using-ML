{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "bd8fe5fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThis code is written as part of COMP8755- Individual Computing Project\\n\\nThis code is for preprocessing and training an LSTM network\\n\\nThe songs are first parsed and the notes of different instruments are procesed to train and mdh5 files for music generation.\\n\\n\\n@Author:\\nMithun\\nu6849970\\n\\n@ Supervisor:\\nProfessor Nick Birbillis\\n\\n'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "This code is written as part of COMP8755- Individual Computing Project\n",
    "\n",
    "This code is for preprocessing and training an LSTM network\n",
    "\n",
    "The songs are first parsed and the notes of different instruments are procesed to train and mdh5 files for music generation.\n",
    "\n",
    "\n",
    "@Author:\n",
    "Mithun\n",
    "u6849970\n",
    "\n",
    "@ Supervisor:\n",
    "Professor Nick Birbillis\n",
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "5ea69f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import *\n",
    "import os\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from keras.layers import *\n",
    "from keras.models import Sequential\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "7ac2cfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function is for reading the midi files which we feed as a data set and \n",
    "parse it into 3 categories, chords, rests and notes\n",
    "\"\"\"\n",
    "def midi_read(file):\n",
    "    print(\"Loading File:\",file)\n",
    "    \n",
    "    note=[]\n",
    "    notes_to_parse = None\n",
    "    rest = True\n",
    "    #parsing a midi file\n",
    "    midi = converter.parse(file)\n",
    "    song = instrument.partitionByInstrument(midi)\n",
    "    print(\"Number of instrument parts: \" + str(len(s2.parts)))\n",
    "    for part in song.parts:\n",
    "          \n",
    "        notes_to_parse = part.recurse() \n",
    "      \n",
    "        #finding whether a particular element is note or a chord    \n",
    "        for element in notes_to_parse:                \n",
    "                #note\n",
    "            if isinstance(element, note.Note):\n",
    "                note.append(str(element.pitch))\n",
    "                #rest\n",
    "            elif isinstance(element, note.Rest) and rest:\n",
    "                note.append(\"rest\")                \n",
    "                #chord\n",
    "            elif isinstance(element, chord.Chord):\n",
    "                note.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n",
    "    return np.array(note)\n",
    "\n",
    "    return note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "4128cd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Music File: midi/Recognizer.mid\n",
      "Number of instrument parts: 7\n",
      "Loading Music File: midi/Daft_Punk_Medley_Pentatonix_Full_Arrangement_SATB.mid\n",
      "Number of instrument parts: 2\n",
      "Loading Music File: midi/GiorgiobyMoroder.mid\n",
      "Number of instrument parts: 1\n",
      "Loading Music File: midi/AroundTheWorld.mid\n",
      "Number of instrument parts: 5\n",
      "Loading Music File: midi/TheGrid.mid\n",
      "Number of instrument parts: 4\n",
      "Loading Music File: midi/FlynnLives.mid\n",
      "Number of instrument parts: 11\n",
      "Loading Music File: midi/Harder_Better_Faster_Stronger_-_Daft_Punk.mscz.mid\n",
      "Number of instrument parts: 1\n",
      "Loading Music File: midi/Veridisquo.mid\n",
      "Number of instrument parts: 5\n",
      "Loading Music File: midi/Contact.mid\n",
      "Number of instrument parts: 7\n",
      "Loading Music File: midi/SomethingAboutUs.mid\n",
      "Number of instrument parts: 4\n",
      "Loading Music File: midi/ShortCircuit.mid\n",
      "Number of instrument parts: 6\n",
      "Loading Music File: midi/RobotRock.mid\n",
      "Number of instrument parts: 4\n",
      "Loading Music File: midi/Aerodynamic(2).mid\n",
      "Number of instrument parts: 5\n",
      "Loading Music File: midi/Daft_Punk-_Get_Lucky_Easy.mid\n",
      "Number of instrument parts: 1\n",
      "Loading Music File: midi/EndofLine.mid\n",
      "Number of instrument parts: 3\n",
      "Loading Music File: midi/Daft_Punk_-_Within.mid\n",
      "Number of instrument parts: 5\n",
      "Loading Music File: midi/GetluckyfeatPharrellWilliams.mid\n",
      "Number of instrument parts: 7\n",
      "Loading Music File: midi/Daft_Punk_-_One_more_Time.mid\n",
      "Number of instrument parts: 1\n",
      "Loading Music File: midi/TelevisionRulesTheNation.mid\n",
      "Number of instrument parts: 4\n",
      "Loading Music File: midi/DaFunk.mid\n",
      "Number of instrument parts: 5\n",
      "Loading Music File: midi/Daft_Punk_-_Touch_Full_Song.mid\n",
      "Number of instrument parts: 16\n",
      "Loading Music File: midi/HarderBetterFasterStronger.mid\n",
      "Number of instrument parts: 7\n",
      "Loading Music File: midi/TheSonOfFlyn.mid\n",
      "Number of instrument parts: 1\n",
      "Loading Music File: midi/Voyager.mid\n",
      "Number of instrument parts: 3\n",
      "Loading Music File: midi/TronLegacyEndTitle.mid\n",
      "Number of instrument parts: 5\n",
      "Loading Music File: midi/Daft_Punk_Pentatonix.mid\n",
      "Number of instrument parts: 1\n",
      "Loading Music File: midi/HumanAfterAll.mid\n",
      "Number of instrument parts: 3\n",
      "Loading Music File: midi/OneMoreTime.mid\n",
      "Number of instrument parts: 7\n",
      "Loading Music File: midi/DigitalLove.mid\n",
      "Number of instrument parts: 1\n",
      "Loading Music File: midi/DigitalLove(2).mid\n",
      "Number of instrument parts: 4\n",
      "Loading Music File: midi/Drezzed.mid\n",
      "Number of instrument parts: 6\n",
      "Loading Music File: midi/Derezzed.mid\n",
      "Number of instrument parts: 1\n",
      "Loading Music File: midi/Daft_Punk_-_Harder_Better_Faster_Stronger_.mid\n",
      "Number of instrument parts: 1\n",
      "Loading Music File: midi/Outlands.mid\n",
      "Number of instrument parts: 9\n",
      "Loading Music File: midi/BackToMyOldSmokyMountains.mid\n",
      "Number of instrument parts: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "path='midi/'\n",
    "\n",
    "#read all the filenames\n",
    "files=[i for i in os.listdir(path) if i.endswith(\".mid\")]\n",
    "\n",
    "\n",
    "#reading each midi file\n",
    "notes_array = np.array([read_midi(path+i) for i in files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "d72625b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302\n"
     ]
    }
   ],
   "source": [
    "#converting 2D array into a vector\n",
    "notes = [element for note_ in notes_array for element in note_]\n",
    "for note_ in notes_array:\n",
    "    for element in note_:\n",
    "        notes.append(element)\n",
    "   \n",
    "#No. of unique notes\n",
    "uniqueNotes = list(set(notes))\n",
    "print(len(uniqueNotes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "7c49941d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252\n"
     ]
    }
   ],
   "source": [
    "#we use the frequent notes to generate music\n",
    "freq = dict(Counter(notes_))\n",
    "\n",
    "for note_,count in freq.items():\n",
    "    if count>=50:\n",
    "        frequent_notes.append(note_)\n",
    "#frequent_notes = [note_ for note_, count in freq.items() if count>=60]\n",
    "print(len(frequent_notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "8c2f9a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "newMusic=[]\n",
    "\n",
    "for notes in notes_array:\n",
    "    temp = [note for note in notes if note in frequent_notes]\n",
    "    \"\"\"for note in notes:\n",
    "        if note in frequent_notes:\n",
    "            temp.append(note)  \"\"\"          \n",
    "    newMusic.append(temp)\n",
    "    \n",
    "newMusic = np.array(newMusic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "15e6da3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#timesteps is a function to tell the number of input notes given for generating the output\n",
    "no_of_timesteps = 25\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for note_ in new_music:\n",
    "    for i in range(0, len(note_) - no_of_timesteps, 1):\n",
    "        \n",
    "        #preparing input and output sequences\n",
    "        input_ = note_[i:i + no_of_timesteps]\n",
    "        output = note_[i + no_of_timesteps]\n",
    "        \n",
    "        x.append(input_)\n",
    "        y.append(output)\n",
    "        \n",
    "x=np.array(x)\n",
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "6248f5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshaping array to 1D\n",
    "unique_x = list(set(x.ravel()))\n",
    "x_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "dfd9fea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing input sequences\n",
    "x_seq=[]\n",
    "for i in x:\n",
    "    temp=[x_note_to_int[j] for j in i]\n",
    "    x_seq.append(temp)\n",
    "    \n",
    "x_seq = np.array(x_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "414c916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_y = list(set(y))\n",
    "y_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_y)) \n",
    "y_seq=np.array([y_note_to_int[i] for i in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "05476a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_tr, x_val, y_tr, y_val = train_test_split(x_seq,y_seq,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ad5bfe93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwe need to create the network back so that we can predict the next set of notes for generating music\\n'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "we need to create the network back so that we can predict the next set of notes for generating music\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "46f74e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Single layer LSTM model with activation as softmax and optimizer as adam\n",
    "def lstm():\n",
    "   model = Sequential()\n",
    "   model.add(Dropout(0.3))\n",
    "   model.add(LSTM(512, return_sequences=True))\n",
    "   model.add(Dropout(0.3))\n",
    "   model.add(Dense(n_vocab))\n",
    "   model.add(Activation('softmax'))\n",
    "   model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "   return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "39baa2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is the checkpint for training the model, after every epoch,\n",
    "the model is updated. We can stop the model once the loss value has saturated\n",
    "\"\"\"\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "mc= ModelCheckpoint('best_model.h5',monitor='val_loss',mode='min',save_best_only=True,verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a156b5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 77/128 [=================>............] - ETA: 3s - loss: 0.8233"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-4e3f70e494e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(np.array(x_tr),np.array(y_tr),batch_size=128,epochs=100, validation_data=(np.array(x_val),np.array(y_val)),verbose=1, callbacks=[mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "51575c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis function generates the notes in a sequential pattern which is then converted to MIDI file\\nwe pick a random sequence from the input and use it as a starting point for generation\\n'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This function generates the notes in a sequential pattern which is then converted to MIDI file\n",
    "we pick a random sequence from the input and use it as a starting point for generation\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2584765e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 32) for input KerasTensor(type_spec=TensorSpec(shape=(None, 32), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 25).\n",
      "[65, 43, 24, 43, 43, 26, 21, 22, 43, 7, 43, 43, 21, 7, 68, 15, 22, 22, 22, 70, 54, 54, 54, 3, 57, 7, 54, 60, 54, 50, 50, 56, 48, 12, 48, 62, 62, 62, 19, 54, 54, 64, 64, 54, 54, 54, 30, 72, 72, 64, 64, 64, 52, 52, 63, 66, 66, 43, 57, 57, 57, 43, 47, 63, 43, 43, 30, 54, 54, 70, 41, 7, 54, 54, 48, 21, 14, 54, 7, 46, 30, 46, 46, 7, 7, 21, 7, 45, 7, 39, 7, 54, 54, 26, 54, 54, 3, 3, 22, 70, 65, 50, 7, 50, 7, 18, 50, 70, 46, 54, 50, 30, 14, 14, 14, 54, 7, 52, 14, 7, 50, 7, 3, 48, 3, 70, 7, 7, 3, 3, 50, 50, 50, 50, 50, 50, 50, 3, 50, 50, 50, 50, 50, 50, 3, 3, 48, 7, 46, 11, 7, 52, 7, 43, 7, 7, 7, 65, 7, 7, 7, 50, 66, 50, 50, 50, 70, 50, 50, 3, 50, 3, 3, 3, 3, 50, 54, 30, 54, 54, 54, 54, 7, 45, 22, 7, 43, 43, 43, 43, 30, 30, 30, 30, 43, 64, 54, 30, 7, 54, 54, 7, 7, 54, 54, 46, 46, 11, 7, 46, 7, 7, 7, 7, 7, 7, 24, 3, 54, 57, 54, 54, 57, 54, 50, 3, 3, 30, 50, 50, 66, 50, 50, 50, 54, 48, 50, 24, 54, 54, 66, 43, 54, 54, 54, 54, 30, 5, 22, 63, 63, 22, 22, 64, 52, 64, 64, 64, 52, 52, 64, 30, 43, 57, 57, 7, 71, 57, 21, 12, 12, 60, 21, 65, 12, 26, 12, 21, 19, 65, 19, 3, 0, 43, 19, 3, 1, 1, 19, 19, 21, 65, 65, 7, 7, 7, 65, 57, 70, 7, 39, 70, 50, 21, 47, 48, 48, 3, 48, 50, 48, 48, 48, 3, 48, 48, 1, 21, 48, 48, 50, 3, 13, 48, 48, 7, 1, 48, 25, 47, 57, 47, 8, 43, 47, 57, 57, 57, 13, 19, 19, 1, 48, 19, 50, 19, 50, 19, 50, 3, 48, 50, 50, 48, 48, 65, 21, 43, 7, 21, 7, 7, 11, 71, 41, 71, 71, 71, 71, 71, 7, 11, 11, 19, 50, 19, 11, 67, 15, 7, 48, 21, 10, 67, 57, 43, 43, 43, 71, 71, 43, 43, 7, 71, 71, 71, 47, 71, 57, 57, 57, 57, 57, 57, 57, 57, 57, 62, 73, 1, 1, 62, 1, 57, 21, 48, 48, 50, 65, 50, 50, 19, 19, 3, 21, 7, 19, 15, 15, 15, 19, 19, 48, 48, 56, 73, 48, 19, 48, 48, 19, 38, 19, 4, 57, 4, 4, 44, 48, 38, 38, 44, 37, 38, 73, 38, 38, 38, 38, 57, 57, 66, 38, 38, 57, 66, 57, 14, 14, 14, 14, 38, 47, 57, 57, 57, 57, 64, 67, 67, 14, 14, 42, 14, 73, 19, 14, 48, 57, 47, 73, 1, 1, 1, 14, 14, 14, 14, 21, 14]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "ind = np.random.randint(0,len(x_val)-1)\n",
    "\"\"\"Random number for prediction\n",
    "\"\"\"\n",
    "random_music = x_val[ind]\n",
    "\n",
    "predictions=[]\n",
    "\"\"\"\n",
    "a total of 150 notes are being generated for our dataset,\n",
    "this value can be changed depending on the dataset.\n",
    "\n",
    "\"\"\"\n",
    "sequence_length = 500\n",
    "for i in range (sequence_length):\n",
    "\n",
    "    random_music = random_music.reshape(1,no_of_timesteps)\n",
    "\n",
    "    #from the prediction input, we use the keras libary for the same\n",
    "\n",
    "    prob  = model.predict(random_music)[0]\n",
    "    y_pred= np.argmax(prob,axis=0)\n",
    "    #The pattern will contain the musical notes which is stored as an array\n",
    "    predictions.append(y_pred)\n",
    "\n",
    "    random_music = np.insert(random_music[0],len(random_music[0]),y_pred)\n",
    "    random_music = random_music[1:]\n",
    "    \n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "df6b7b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_int_to_note = dict((number, note_) for number, note_ in enumerate(unique_x)) \n",
    "predicted_notes = [x_int_to_note[i] for i in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "50cd8612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGenerating the Midi file from prediction\\n'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Generating the Midi file from prediction\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b7503bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "â€œWe used the Generate Piano Instrumental Music by Using Deep Learning(2019) by Haryo Akbarianto Wibowo to complete our work.\"\n",
    "\n",
    "\"\"\"\"\n",
    "\n",
    "def create_midi(prediction_output):\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    \n",
    "\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "    for pattern in prediction_output:\n",
    "        # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                new_note = note.Note(int(current_note))\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "        elif pattern == 'rest':\n",
    "            new_note = note.Rest()\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "        # pattern is a note\n",
    "        else:\n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += 0.25\n",
    "\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "\n",
    "    midi_stream.write('midi', fp='music.mid')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1946e0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_midi(predicted_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412f4188",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-cpu.2-4.m68",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-4:m68"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
